{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 16])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# x sequence in {1..8}\n",
    "sentence = torch.tensor([0, 7, 1, 2, 5, 6, 4, 3])\n",
    "torch.manual_seed(123)\n",
    "embed = torch.nn.Embedding(10, 16)\n",
    "embedded_sentence = embed(sentence)\n",
    "embedded_sentence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 9.7601,  1.7326,  4.7543, -1.3587,  0.4752, -1.6717,  1.0227, -0.1286],\n",
      "        [ 1.7326, 16.0787,  9.0642, -0.3370,  1.1368,  1.1972,  1.6485, -1.2789],\n",
      "        [ 4.7543,  9.0642, 22.6615, -0.8519,  7.7799,  2.7483, -0.6832,  1.6236],\n",
      "        [-1.3587, -0.3370, -0.8519, 13.9473, -1.4198, 10.9659, -0.5887,  2.3869],\n",
      "        [ 0.4752,  1.1368,  7.7799, -1.4198, 13.7511, -6.8568, -2.5114, -3.3468],\n",
      "        [-1.6717,  1.1972,  2.7483, 10.9659, -6.8568, 24.6738, -3.8294,  4.9581],\n",
      "        [ 1.0227,  1.6485, -0.6832, -0.5887, -2.5114, -3.8294, 15.8691,  2.0269],\n",
      "        [-0.1286, -1.2789,  1.6236,  2.3869, -3.3468,  4.9581,  2.0269, 18.7382]],\n",
      "       grad_fn=<CopySlices>)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "omega = torch.zeros(8, 8)\n",
    "for i, x_i in enumerate(embedded_sentence):\n",
    "    for j, x_j in enumerate(embedded_sentence):\n",
    "        omega[i, j] = torch.dot(x_i, x_j)\n",
    "print(omega)\n",
    "# embedded_sentence is 8x16, transpose: 16x8\n",
    "# x_i的形式是行向量，每个行下标代表一个输入，即x_i\n",
    "omega_mat = torch.matmul(embedded_sentence, embedded_sentence.T)\n",
    "print(torch.allclose(omega, omega_mat))\n",
    "import torch.nn.functional as F\n",
    "# 在omega_i_j中，我们对j分量进行归一化\n",
    "attention_weights = F.softmax(omega, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-9.3975e-01, -4.6856e-01,  1.0311e+00, -2.8192e-01,  4.9373e-01,\n",
      "        -1.2896e-02, -2.7327e-01, -7.6358e-01,  1.3958e+00, -9.9543e-01,\n",
      "        -7.1287e-04,  1.2449e+00, -7.8077e-02,  1.2765e+00, -1.4589e+00,\n",
      "        -2.1601e+00], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x_2 = embedded_sentence[1, :]\n",
    "context_vec_2 = torch.zeros(x_2.shape)\n",
    "for j in range(8):\n",
    "    x_j = embedded_sentence[j, :]\n",
    "    context_vec_2 += attention_weights[1, j] * x_j\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.3420e-01, -1.8324e-01, -3.0218e-01, -5.7772e-01,  3.5662e-01,\n",
      "          6.6452e-01, -2.0998e-01, -3.7798e-01,  7.6537e-01, -1.1946e+00,\n",
      "          6.9960e-01, -1.4067e+00,  1.7021e-01,  1.8838e+00,  4.8729e-01,\n",
      "          2.4730e-01],\n",
      "        [-9.3975e-01, -4.6856e-01,  1.0311e+00, -2.8192e-01,  4.9373e-01,\n",
      "         -1.2896e-02, -2.7327e-01, -7.6358e-01,  1.3958e+00, -9.9543e-01,\n",
      "         -7.1287e-04,  1.2449e+00, -7.8077e-02,  1.2765e+00, -1.4589e+00,\n",
      "         -2.1601e+00],\n",
      "        [-7.7021e-02, -1.0205e+00, -1.6895e-01,  9.1776e-01,  1.5810e+00,\n",
      "          1.3010e+00,  1.2753e+00, -2.0095e-01,  4.9647e-01, -1.5723e+00,\n",
      "          9.6657e-01, -1.1481e+00, -1.1589e+00,  3.2547e-01, -6.3151e-01,\n",
      "         -2.8400e+00],\n",
      "        [-1.3679e+00,  1.0614e-01, -2.1317e+00,  1.0480e+00, -3.7127e-01,\n",
      "         -9.1234e-01, -4.3802e-01, -1.0329e+00,  9.3425e-01,  1.5453e+00,\n",
      "          5.7218e-01, -1.8049e-01, -6.0454e-03, -8.8691e-02,  2.0559e-01,\n",
      "         -5.2292e-01],\n",
      "        [ 2.5444e-01, -5.5082e-01,  1.0012e+00,  8.2746e-01, -3.8978e-01,\n",
      "          4.9129e-01, -2.1302e-01, -1.7432e+00, -1.5972e+00, -1.0776e+00,\n",
      "          9.0331e-01, -7.2292e-01, -5.9652e-01, -7.0857e-01,  6.1977e-01,\n",
      "         -1.3766e+00],\n",
      "        [-2.2150e+00, -1.3193e+00, -2.0915e+00,  9.6285e-01, -3.1862e-02,\n",
      "         -4.7896e-01,  7.6681e-01,  2.7467e-02,  1.9929e+00,  1.3708e+00,\n",
      "         -5.0087e-01, -2.7928e-01, -2.0628e+00,  6.3744e-03, -9.8955e-01,\n",
      "          7.0161e-01],\n",
      "        [ 5.1463e-01,  9.9376e-01, -2.5873e-01, -1.0825e+00, -4.4383e-02,\n",
      "          1.6236e+00, -2.3229e+00,  1.0878e+00,  6.7156e-01,  6.9329e-01,\n",
      "         -9.4872e-01, -7.6506e-02, -1.5264e-01,  1.1674e-01,  4.4026e-01,\n",
      "         -1.4465e+00],\n",
      "        [ 8.7683e-01,  1.6221e+00, -1.4779e+00,  1.1331e+00, -1.2203e+00,\n",
      "          1.3139e+00,  1.0533e+00,  1.3881e-01,  2.2473e+00, -8.0363e-01,\n",
      "         -2.8084e-01,  7.6968e-01, -6.5956e-01, -7.9793e-01,  1.8383e-01,\n",
      "          2.2935e-01]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "context_vectors = torch.matmul(attention_weights, embedded_sentence)\n",
    "print(context_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "d = embedded_sentence.shape[1]\n",
    "U_query = torch.rand(d, d)\n",
    "U_key = torch.rand(d, d)\n",
    "U_value = torch.rand(d, d)\n",
    "x_2 = embedded_sentence[1]\n",
    "query_2 = U_query.matmul(x_2)\n",
    "key_2 = U_key.matmul(x_2)\n",
    "value_2 = U_value.matmul(x_2)\n",
    "keys = U_key.matmul(embedded_sentence.T).T\n",
    "values = (U_value @ embedded_sentence.T).T\n",
    "print(torch.allclose(keys[1], key_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14.3667, grad_fn=<DotBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omega_23 = query_2 @ keys[2]\n",
    "omega_23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-25.1623,   9.3602,  14.3667,  32.1482,  53.8976,  46.6626,  -1.2131,\n",
      "        -32.9392], grad_fn=<SqueezeBackward4>)\n",
      "tensor([[ -0.7569,  -3.7951,  -7.9465, -10.0615, -12.1732, -12.8006,   4.1644,\n",
      "           6.3346],\n",
      "        [-25.1623,   9.3602,  14.3667,  32.1482,  53.8976,  46.6626,  -1.2131,\n",
      "         -32.9392],\n",
      "        [-28.8096,  10.9046,  14.4355,  23.8255,  52.7999,  41.3237,   1.5884,\n",
      "         -35.1890],\n",
      "        [-15.5115,  17.5500,  19.8771,  21.5002,  42.0597,  35.2061,  -0.5541,\n",
      "         -25.9203],\n",
      "        [-36.3682,  20.2438,  27.1240,  49.8610,  84.9364,  85.7472,   5.8265,\n",
      "         -69.9103],\n",
      "        [-34.6901,  38.3814,  42.0269,  48.1298,  92.0512,  74.9869,  -6.6510,\n",
      "         -65.5576],\n",
      "        [ -1.1880,   3.7619,  -5.6129,  -6.8691,   6.3126, -13.3452,  -1.3225,\n",
      "          -6.2390],\n",
      "        [ 31.8297, -25.2041, -25.3536, -57.8440, -79.4676, -85.3054, -10.5390,\n",
      "          64.5980]], grad_fn=<MmBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omega_2 = query_2 @ keys.T\n",
    "print(omega_2)\n",
    "queries = (U_query @ embedded_sentence.T).T\n",
    "omega = (queries @ keys.T)\n",
    "print(omega)\n",
    "torch.allclose(omega_2, omega[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "X = embedded_sentence.T\n",
    "Q = U_query @ X\n",
    "K = U_key @ X\n",
    "omega = Q.T @ K\n",
    "print(torch.allclose(omega_23, omega[1, 2])) # True\n",
    "query_2 = U_query @ x_2 # x_2 is a rank-1 tensor, we regard as dx1 vector\n",
    "omega_2 = query_2 @ keys.T\n",
    "print(torch.allclose(omega_2, omega[1, :])) # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.2317e-09, 1.2499e-05, 4.3696e-05, 3.7242e-03, 8.5596e-01, 1.4026e-01,\n",
      "        8.8897e-07, 3.1935e-10], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([-1.2226, -3.4387, -4.3928, -5.2125, -1.1249, -3.3041, -1.4316, -3.2765,\n",
      "        -2.5114, -2.6105, -1.5793, -2.8433, -2.4142, -0.3998, -1.9917, -3.3499],\n",
      "       grad_fn=<SqueezeBackward4>)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "attention_weights_2 = F.softmax(omega_2 / d**5e-1, dim=0)\n",
    "print(attention_weights_2)\n",
    "context_vector_2 = attention_weights_2 @ values\n",
    "print(context_vector_2)\n",
    "print(torch.allclose(context_vector_2, values.T @ attention_weights_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "torch.Size([8, 8])\n"
     ]
    }
   ],
   "source": [
    "attention_weights = F.softmax(omega / d**5e-1, dim=1)\n",
    "context_vectors = attention_weights @ values\n",
    "print(\n",
    "    torch.allclose(context_vectors[1], context_vector_2)\n",
    ")\n",
    "print(attention_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6,  9, 12])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.tensor([1, 2])\n",
    "t2 = torch.arange(0, 6)\n",
    "t2 = t2.reshape((2, 3))\n",
    "t = t1 @ t2\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 16])\n",
      "torch.Size([16])\n",
      "torch.Size([8, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(123)\n",
    "d = embedded_sentence.shape[1]\n",
    "one_U_query = torch.rand(d, d)\n",
    "h = 8\n",
    "multihead_U_query = torch.rand(h, d, d)\n",
    "multihead_U_key = torch.rand(h, d, d)\n",
    "multihead_U_value = torch.rand(h, d, d)\n",
    "\n",
    "multihead_query_2 = multihead_U_query @ x_2\n",
    "print(multihead_query_2.shape)\n",
    "print(x_2.shape)\n",
    "print(multihead_U_query.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.9619, -0.7701, -0.7280, -1.6840, -1.0801, -1.6778,  0.6763,  0.6547,\n",
       "         1.4445, -2.7016, -1.1364, -1.1204, -2.4430, -0.5982, -0.8292, -1.4401],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multihead_key_2 = multihead_U_key @ x_2\n",
    "multihead_value_2 = multihead_U_value @ x_2\n",
    "multihead_key_2[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.3737e-01, -9.4053e-01, -7.7020e-02, -1.3250e+00,  2.5529e-01,\n",
      "         -2.2150e+00,  5.1463e-01,  8.7684e-01],\n",
      "        [-1.7778e-01, -4.6806e-01, -1.0205e+00,  1.7843e-01, -5.4963e-01,\n",
      "         -1.3193e+00,  9.9376e-01,  1.6221e+00],\n",
      "        [-3.0353e-01,  1.0322e+00, -1.6896e-01, -2.1338e+00,  1.0042e+00,\n",
      "         -2.0915e+00, -2.5873e-01, -1.4779e+00],\n",
      "        [-5.8801e-01, -2.8300e-01,  9.1776e-01,  1.0524e+00,  8.2723e-01,\n",
      "          9.6285e-01, -1.0826e+00,  1.1331e+00],\n",
      "        [ 3.4861e-01,  4.9275e-01,  1.5810e+00, -3.8848e-01, -3.9481e-01,\n",
      "         -3.1861e-02, -4.4382e-02, -1.2203e+00],\n",
      "        [ 6.6034e-01, -1.4078e-02,  1.3010e+00, -9.3435e-01,  4.8923e-01,\n",
      "         -4.7896e-01,  1.6236e+00,  1.3139e+00],\n",
      "        [-2.1964e-01, -2.7466e-01,  1.2753e+00, -4.9914e-01, -2.1681e-01,\n",
      "          7.6681e-01, -2.3229e+00,  1.0533e+00],\n",
      "        [-3.7917e-01, -7.6409e-01, -2.0095e-01, -1.0867e+00, -1.7472e+00,\n",
      "          2.7468e-02,  1.0878e+00,  1.3881e-01],\n",
      "        [ 7.6711e-01,  1.3966e+00,  4.9647e-01,  8.8054e-01, -1.6025e+00,\n",
      "          1.9929e+00,  6.7155e-01,  2.2473e+00],\n",
      "        [-1.1925e+00, -9.9491e-01, -1.5723e+00,  1.5542e+00, -1.0764e+00,\n",
      "          1.3708e+00,  6.9330e-01, -8.0364e-01],\n",
      "        [ 6.9835e-01, -1.5822e-03,  9.6657e-01,  6.2662e-01,  9.0315e-01,\n",
      "         -5.0087e-01, -9.4872e-01, -2.8084e-01],\n",
      "        [-1.4097e+00,  1.2471e+00, -1.1481e+00, -1.7549e-01, -7.2184e-01,\n",
      "         -2.7928e-01, -7.6507e-02,  7.6968e-01],\n",
      "        [ 1.7938e-01, -7.7105e-02, -1.1589e+00,  9.8284e-02, -5.9508e-01,\n",
      "         -2.0628e+00, -1.5264e-01, -6.5956e-01],\n",
      "        [ 1.8951e+00,  1.2774e+00,  3.2547e-01, -9.3507e-02, -7.1122e-01,\n",
      "          6.3745e-03,  1.1674e-01, -7.9793e-01],\n",
      "        [ 4.9545e-01, -1.4596e+00, -6.3151e-01,  2.6621e-01,  6.2296e-01,\n",
      "         -9.8955e-01,  4.4026e-01,  1.8383e-01],\n",
      "        [ 2.6920e-01, -2.1595e+00, -2.8400e+00, -5.8504e-01, -1.3729e+00,\n",
      "          7.0161e-01, -1.4465e+00,  2.2935e-01]], grad_fn=<PermuteBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedded_sentence.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.3374, -0.9405, -0.0770,  ..., -2.2150,  0.5146,  0.8768],\n",
      "         [-0.1778, -0.4681, -1.0205,  ..., -1.3193,  0.9938,  1.6221],\n",
      "         [-0.3035,  1.0322, -0.1690,  ..., -2.0915, -0.2587, -1.4779],\n",
      "         ...,\n",
      "         [ 1.8951,  1.2774,  0.3255,  ...,  0.0064,  0.1167, -0.7979],\n",
      "         [ 0.4954, -1.4596, -0.6315,  ..., -0.9896,  0.4403,  0.1838],\n",
      "         [ 0.2692, -2.1595, -2.8400,  ...,  0.7016, -1.4465,  0.2293]],\n",
      "\n",
      "        [[ 0.3374, -0.9405, -0.0770,  ..., -2.2150,  0.5146,  0.8768],\n",
      "         [-0.1778, -0.4681, -1.0205,  ..., -1.3193,  0.9938,  1.6221],\n",
      "         [-0.3035,  1.0322, -0.1690,  ..., -2.0915, -0.2587, -1.4779],\n",
      "         ...,\n",
      "         [ 1.8951,  1.2774,  0.3255,  ...,  0.0064,  0.1167, -0.7979],\n",
      "         [ 0.4954, -1.4596, -0.6315,  ..., -0.9896,  0.4403,  0.1838],\n",
      "         [ 0.2692, -2.1595, -2.8400,  ...,  0.7016, -1.4465,  0.2293]],\n",
      "\n",
      "        [[ 0.3374, -0.9405, -0.0770,  ..., -2.2150,  0.5146,  0.8768],\n",
      "         [-0.1778, -0.4681, -1.0205,  ..., -1.3193,  0.9938,  1.6221],\n",
      "         [-0.3035,  1.0322, -0.1690,  ..., -2.0915, -0.2587, -1.4779],\n",
      "         ...,\n",
      "         [ 1.8951,  1.2774,  0.3255,  ...,  0.0064,  0.1167, -0.7979],\n",
      "         [ 0.4954, -1.4596, -0.6315,  ..., -0.9896,  0.4403,  0.1838],\n",
      "         [ 0.2692, -2.1595, -2.8400,  ...,  0.7016, -1.4465,  0.2293]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.3374, -0.9405, -0.0770,  ..., -2.2150,  0.5146,  0.8768],\n",
      "         [-0.1778, -0.4681, -1.0205,  ..., -1.3193,  0.9938,  1.6221],\n",
      "         [-0.3035,  1.0322, -0.1690,  ..., -2.0915, -0.2587, -1.4779],\n",
      "         ...,\n",
      "         [ 1.8951,  1.2774,  0.3255,  ...,  0.0064,  0.1167, -0.7979],\n",
      "         [ 0.4954, -1.4596, -0.6315,  ..., -0.9896,  0.4403,  0.1838],\n",
      "         [ 0.2692, -2.1595, -2.8400,  ...,  0.7016, -1.4465,  0.2293]],\n",
      "\n",
      "        [[ 0.3374, -0.9405, -0.0770,  ..., -2.2150,  0.5146,  0.8768],\n",
      "         [-0.1778, -0.4681, -1.0205,  ..., -1.3193,  0.9938,  1.6221],\n",
      "         [-0.3035,  1.0322, -0.1690,  ..., -2.0915, -0.2587, -1.4779],\n",
      "         ...,\n",
      "         [ 1.8951,  1.2774,  0.3255,  ...,  0.0064,  0.1167, -0.7979],\n",
      "         [ 0.4954, -1.4596, -0.6315,  ..., -0.9896,  0.4403,  0.1838],\n",
      "         [ 0.2692, -2.1595, -2.8400,  ...,  0.7016, -1.4465,  0.2293]],\n",
      "\n",
      "        [[ 0.3374, -0.9405, -0.0770,  ..., -2.2150,  0.5146,  0.8768],\n",
      "         [-0.1778, -0.4681, -1.0205,  ..., -1.3193,  0.9938,  1.6221],\n",
      "         [-0.3035,  1.0322, -0.1690,  ..., -2.0915, -0.2587, -1.4779],\n",
      "         ...,\n",
      "         [ 1.8951,  1.2774,  0.3255,  ...,  0.0064,  0.1167, -0.7979],\n",
      "         [ 0.4954, -1.4596, -0.6315,  ..., -0.9896,  0.4403,  0.1838],\n",
      "         [ 0.2692, -2.1595, -2.8400,  ...,  0.7016, -1.4465,  0.2293]]],\n",
      "       grad_fn=<RepeatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "stacked_inputs = embedded_sentence.T.repeat(8, 1, 1)\n",
    "print(stacked_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 1, 2, 1, 2],\n",
      "        [3, 4, 3, 4, 3, 4],\n",
      "        [5, 6, 5, 6, 5, 6],\n",
      "        [1, 2, 1, 2, 1, 2],\n",
      "        [3, 4, 3, 4, 3, 4],\n",
      "        [5, 6, 5, 6, 5, 6]])\n",
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6],\n",
      "        [1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6],\n",
      "        [1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "tensor([[[1, 2, 1, 2, 1, 2],\n",
      "         [3, 4, 3, 4, 3, 4],\n",
      "         [5, 6, 5, 6, 5, 6],\n",
      "         [1, 2, 1, 2, 1, 2],\n",
      "         [3, 4, 3, 4, 3, 4],\n",
      "         [5, 6, 5, 6, 5, 6]],\n",
      "\n",
      "        [[1, 2, 1, 2, 1, 2],\n",
      "         [3, 4, 3, 4, 3, 4],\n",
      "         [5, 6, 5, 6, 5, 6],\n",
      "         [1, 2, 1, 2, 1, 2],\n",
      "         [3, 4, 3, 4, 3, 4],\n",
      "         [5, 6, 5, 6, 5, 6]]])\n"
     ]
    }
   ],
   "source": [
    "# x是一个shape为(3, 2)的Tensor\n",
    "x = torch.tensor([[1, 2], \n",
    "                  [3, 4], \n",
    "                  [5, 6]])\n",
    "#得到shape为(2*3, 3*2)，即(6, 6)的Tensor\n",
    "#将维度从后往前的考察。首先，将x的第二维扩充到3*x.shape[1]维，也就是重复x的第二维3次。\n",
    "#随后，将x的第一维扩充到2*x.shape[0]维，扩充部分是repeat；\n",
    "print(x.repeat(2, 3))\n",
    "#再看一个例子，第一维重复扩充了3背，第二维保持。所以得到shape为(6,2)。\n",
    "print(x.repeat(3, 1)) # 得到一个rank-2，每个保留了第二维，重复了2次第一维。\n",
    "#对于扩充到高维的情况，倒数第一维相当于扩充了x的倒数第一维的3倍，采用重复的策略；\n",
    "#倒数第二维将x的倒数第二维按照重复的策略扩充2倍；\n",
    "#由于x没有倒数第三维，所以将已经重复扩充的结果，repeat，重复之。\n",
    "print(x.repeat(2, 2, 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
