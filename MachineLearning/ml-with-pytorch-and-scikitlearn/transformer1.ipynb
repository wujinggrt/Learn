{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim=256, num_heads=4):\n",
    "        \"\"\"\n",
    "        hidden_dim: 输入的维度\n",
    "        num_heads: 输入分成的注意头数量\n",
    "\n",
    "        维护的是\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        assert hidden_dim % num_heads == 0, \"hidden_dim must be the integer times of num_heads\"\n",
    "        # query, key, value, and output\n",
    "        self.Wq = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.Wk = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.Wv = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.Wo = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "\n",
    "    def _check_scaled_dot_product_attention_inputs(self, x):\n",
    "        \"\"\"\n",
    "        check scaled dot-product attention inputs\n",
    "        \"\"\"\n",
    "        assert x.size(1) == self.num_heads, f\"expects that x has shape as:\" \\\n",
    "             f\" ({-1, self.num_heads, -1, self.hidden_dim // self.num_heads}),\" \\\n",
    "             f\"but get {x.size()}\"\n",
    "        assert x.size(3) == self.hidden_dim\n",
    "\n",
    "    def _scaled_dot_product_attention(self, query, key, value, \n",
    "                                      attention_mask=None, key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        query: tensor, shape is (batch_size, num_heads, query_sequence_length, hidden_dim // num_heads)\n",
    "        key: tensor, shape is (batch_size, num_heads, key_sequence_length, hidden_dim // num_heads)\n",
    "        value: tensor, shape is (batch_size, num_heads, value_sequence_length, hidden_dim // num_heads)\n",
    "        attention_mask: tensor, shape is (query_sequence_length, key_sequence_length)\n",
    "        key_padding_mask: tensor, shape is (sequence_length, key_sequence_length)\n",
    "\n",
    "        query最开始是(batch_size, query_sequence_length, hidden_dim)的,w，经过split_into_heads后的结果。\n",
    "        \"\"\"\n",
    "        self.check_scaled_dot_product_attention_inputs(query)\n",
    "        self.check_scaled_dot_product_attention_inputs(key)\n",
    "        self.check_scaled_dot_product_attention_inputs(value)\n",
    "\n",
    "        d_k = key.size(-1)\n",
    "        tgt_len, src_len = query.size(-2), key.size(-2)\n",
    "\n",
    "        # logits = (B, H, tgt_len, E) * (B, H, E, src_len) = (B, H, tgt_len, src_len)\n",
    "        logits = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "        # 注意力遮罩\n",
    "        if attention_mask:\n",
    "            if attention_mask.dim() == 2:\n",
    "                assert attention_mask.size() == (tgt_len, src_len)\n",
    "                # 广播到 (1, query_sequence_length, key_sequence_length)\n",
    "                attention_mask = attention_mask.unsqueeze(0)\n",
    "                logits = logits + attention_mask\n",
    "            else:\n",
    "                raise ValueError(f\"attention_mask.size() is invalid: {attention_mask.size()}\")\n",
    "\n",
    "        if key_padding_mask:\n",
    "            # Broadcast to fit logits\n",
    "            # 广播到 query_sequence_length, 1, 1, key_sequence_length)\n",
    "            key_padding_mask = key_padding_mask.unsqueeze(1).unsqueeze(2)\n",
    "            logits = logits + key_padding_mask\n",
    "        \n",
    "        attention = torch.softmax(logits, dim=-1)\n",
    "        # (batch_size, num_heads, sequence_length, hidden_dim)\n",
    "        output = torch.matmul(attention, value)\n",
    "        return output, attention\n",
    "\n",
    "    def _split_into_heads(self, x, num_heads):\n",
    "        batch_size, seq_length, hidden_dim = x.size()\n",
    "        \"\"\"\n",
    "        我们以q为例：\n",
    "        对于每hidden_dim个元素，我们拆分成(num_heads, d_k)的形式，所以对应着，\n",
    "        源输入的x是hidden_dim个都被Wq转换过，所以\n",
    "        \"\"\"\n",
    "        x = x.view(batch_size, seq_length, num_heads, hidden_dim // num_heads)\n",
    "        # 最终返回(batch_size, num_heads, seq_length, hidden_dim // num_heads)\n",
    "        return x.transpose(1, 2)\n",
    "\n",
    "    # 上一个方法的逆操作\n",
    "    def combine_heads(self, x):\n",
    "        batch_size, num_heads, seq_length, head_hidden_dim = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_length, num_heads * head_hidden_dim\n",
    "        )\n",
    "\n",
    "    def forward(self, q, k, v, attention_mask=None, key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        q: tensor, shape is (batch_size, query_sequence_length, hidden_dim)\n",
    "        k: tensor, shape is (batch_size, key_sequence_length, hidden_dim)\n",
    "        v: tensor, shape is (batch_size, value_sequence_length, hidden_dim)\n",
    "        attention_mask: tensor, shape is (query_sequence_length, key_sequence_length)\n",
    "        key_padding_mask: tensor, shape is (sequence_length, key_sequence_length)\n",
    "        \"\"\"\n",
    "        q = self.Wq(q)\n",
    "        k = self.Wk(k)\n",
    "        v = self.Wv(v)\n",
    "\n",
    "        q = self.split_into_heads(q, self.num_heads)\n",
    "        k = self.split_into_heads(k, self.num_heads)\n",
    "        v = self.split_into_heads(v, self.num_heads)\n",
    "\n",
    "        attention_values, attention_weights = self.scaled_dot_product_attention(\n",
    "            q, k, v, attention_mask, key_padding_mask)\n",
    "        grouped = self.combine_heads(attention_values)\n",
    "        output = self.Wo(grouped)\n",
    "\n",
    "        self.attention_weights = attention_weights\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 20])\n",
      "None\n",
      "torch.Size([4, 2, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "l = nn.Linear(20, 30, bias=False)\n",
    "print(l.weight.shape)\n",
    "print(l.bias)\n",
    "\n",
    "import torch\n",
    "t1 = torch.arange(24).reshape((2, 3, 4)).repeat((4, 1, 1, 1))\n",
    "# t2 = torch.arange(40*2).reshape((2, 2, 4, 5)) # error\n",
    "t2 = torch.arange(40).reshape((2, 4, 5)).repeat((4, 1, 1, 1))\n",
    "t3 = t1 @ t2\n",
    "print(t3.shape)\n",
    "assert(torch.allclose(t3[0, 1], t1[0, 1] @ t2[0, 1]))\n",
    "assert(torch.allclose(t3[0, 1], t1[2, 1] @ t2[0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11, 12, 13, 14, 15],\n",
      "        [16, 17, 18, 19, 20, 21, 22, 23]])\n",
      "tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7],\n",
      "         [ 8,  9, 10, 11]],\n",
      "\n",
      "        [[12, 13, 14, 15],\n",
      "         [16, 17, 18, 19],\n",
      "         [20, 21, 22, 23]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.arange(24).reshape(3, 8)\n",
    "z = x.view(-1, 3, 4)\n",
    "print(x)\n",
    "print(z)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
