{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim=256, num_heads=4):\n",
    "        \"\"\"\n",
    "        hidden_dim: 输入的维度\n",
    "        num_heads: 输入分成的注意头数量\n",
    "\n",
    "        维护的是\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        assert hidden_dim % num_heads == 0, \"hidden_dim must be the integer times of num_heads\"\n",
    "        # query, key, value, and output\n",
    "        self.Wq = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.Wk = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.Wv = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.Wo = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "\n",
    "    def _check_scaled_dot_product_attention_inputs(self, x):\n",
    "        \"\"\"\n",
    "        check scaled dot-product attention inputs\n",
    "        \"\"\"\n",
    "        assert x.size(1) == self.num_heads, f\"expects that x has shape as:\" \\\n",
    "             f\" ({-1, self.num_heads, -1, self.hidden_dim // self.num_heads}),\" \\\n",
    "             f\"but get {x.size()}\"\n",
    "        assert x.size(3) == self.hidden_dim\n",
    "\n",
    "    def _scaled_dot_product_attention(self, query, key, value, \n",
    "                                      attention_mask=None, key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        query: tensor, shape is (batch_size, num_heads, query_sequence_length, hidden_dim // num_heads)\n",
    "        key: tensor, shape is (batch_size, num_heads, key_sequence_length, hidden_dim // num_heads)\n",
    "        value: tensor, shape is (batch_size, num_heads, value_sequence_length, hidden_dim // num_heads)\n",
    "        attention_mask: tensor, shape is (query_sequence_length, key_sequence_length)\n",
    "        key_padding_mask: tensor, shape is (sequence_length, key_sequence_length)\n",
    "\n",
    "        query最开始是(batch_size, query_sequence_length, hidden_dim)的,w，经过split_into_heads后的结果。\n",
    "        \"\"\"\n",
    "        self.check_scaled_dot_product_attention_inputs(query)\n",
    "        self.check_scaled_dot_product_attention_inputs(key)\n",
    "        self.check_scaled_dot_product_attention_inputs(value)\n",
    "\n",
    "        d_k = key.size(-1)\n",
    "        tgt_len, src_len = query.size(-2), key.size(-2)\n",
    "\n",
    "        # logits = (B, H, tgt_len, E) * (B, H, E, src_len) = (B, H, tgt_len, src_len)\n",
    "        logits = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "        # 注意力遮罩\n",
    "        if attention_mask:\n",
    "            if attention_mask.dim() == 2:\n",
    "                assert attention_mask.size() == (tgt_len, src_len)\n",
    "                # 广播到 (1, query_sequence_length, key_sequence_length)\n",
    "                attention_mask = attention_mask.unsqueeze(0)\n",
    "                logits = logits + attention_mask\n",
    "            else:\n",
    "                raise ValueError(f\"attention_mask.size() is invalid: {attention_mask.size()}\")\n",
    "\n",
    "        if key_padding_mask:\n",
    "            # Broadcast to fit logits\n",
    "            # 广播到 query_sequence_length, 1, 1, key_sequence_length)\n",
    "            key_padding_mask = key_padding_mask.unsqueeze(1).unsqueeze(2)\n",
    "            logits = logits + key_padding_mask\n",
    "        \n",
    "        attention = torch.softmax(logits, dim=-1)\n",
    "        # (batch_size, num_heads, sequence_length, hidden_dim)\n",
    "        output = torch.matmul(attention, value)\n",
    "        return output, attention\n",
    "\n",
    "    def _split_into_heads(self, x, num_heads):\n",
    "        batch_size, seq_length, hidden_dim = x.size()\n",
    "        \"\"\"\n",
    "        我们以q为例：\n",
    "        对于每hidden_dim个元素，我们拆分成(num_heads, d_k)的形式，所以对应着，\n",
    "        源输入的x是hidden_dim个都被Wq转换过，所以\n",
    "        \"\"\"\n",
    "        x = x.view(batch_size, seq_length, num_heads, hidden_dim // num_heads)\n",
    "        # 最终返回(batch_size, num_heads, seq_length, hidden_dim // num_heads)\n",
    "        return x.transpose(1, 2)\n",
    "\n",
    "    # 上一个方法的逆操作\n",
    "    def combine_heads(self, x):\n",
    "        batch_size, num_heads, seq_length, head_hidden_dim = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_length, num_heads * head_hidden_dim\n",
    "        )\n",
    "\n",
    "    def forward(self, q, k, v, attention_mask=None, key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        q: tensor, shape is (batch_size, query_sequence_length, hidden_dim)\n",
    "        k: tensor, shape is (batch_size, key_sequence_length, hidden_dim)\n",
    "        v: tensor, shape is (batch_size, value_sequence_length, hidden_dim)\n",
    "        attention_mask: tensor, shape is (query_sequence_length, key_sequence_length)\n",
    "        key_padding_mask: tensor, shape is (sequence_length, key_sequence_length)\n",
    "        \"\"\"\n",
    "        q = self.Wq(q)\n",
    "        k = self.Wk(k)\n",
    "        v = self.Wv(v)\n",
    "\n",
    "        q = self.split_into_heads(q, self.num_heads)\n",
    "        k = self.split_into_heads(k, self.num_heads)\n",
    "        v = self.split_into_heads(v, self.num_heads)\n",
    "\n",
    "        attention_values, attention_weights = self.scaled_dot_product_attention(\n",
    "            q, k, v, attention_mask, key_padding_mask)\n",
    "        grouped = self.combine_heads(attention_values)\n",
    "        output = self.Wo(grouped)\n",
    "\n",
    "        self.attention_weights = attention_weights\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 20])\n",
      "None\n",
      "torch.Size([4, 2, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "l = nn.Linear(20, 30, bias=False)\n",
    "print(l.weight.shape)\n",
    "print(l.bias)\n",
    "\n",
    "import torch\n",
    "t1 = torch.arange(24).reshape((2, 3, 4)).repeat((4, 1, 1, 1))\n",
    "# t2 = torch.arange(40*2).reshape((2, 2, 4, 5)) # error\n",
    "t2 = torch.arange(40).reshape((2, 4, 5)).repeat((4, 1, 1, 1))\n",
    "t3 = t1 @ t2\n",
    "print(t3.shape)\n",
    "assert(torch.allclose(t3[0, 1], t1[0, 1] @ t2[0, 1]))\n",
    "assert(torch.allclose(t3[0, 1], t1[2, 1] @ t2[0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -0.0000,   0.0000,   0.0000,   0.0000,  -0.0000,  -0.0000,  -0.0000,\n",
      "          -7.4608,   0.0000,   0.0000,  -0.0000,  -0.0000,  -0.0000,  -0.0000,\n",
      "          -0.0000,  -5.5937],\n",
      "        [  0.0000,   0.0000,  -0.0000,  -0.0000,   1.6298,   4.2003,  -0.0000,\n",
      "          -0.0000,   7.4670,   0.0000,   0.0000,  -7.8200,  -8.2668,   0.0000,\n",
      "           0.0000,  -0.0000],\n",
      "        [  0.0000,   7.8263,   0.6403,  -0.0000,  -0.0000,  -0.0000,   0.0000,\n",
      "          -0.0000,  -0.0000,  -0.0000,  -0.0000, -10.2492,   3.1188,   0.0000,\n",
      "           0.0000, -10.6701],\n",
      "        [  0.0000,   6.2545,  -0.0000, -10.1728,   0.0000,  -0.0000,  -0.0000,\n",
      "           0.0000,   0.0000,   0.0000,  -0.0000,   0.0000,   7.9080,  -0.6719,\n",
      "           0.0000,  -0.0000],\n",
      "        [  0.0000,  -0.0000,   0.0000,   2.3193,  -0.0000,   0.0000,   0.0000,\n",
      "          -0.0000,   0.0000,   0.0000,  13.2069,   0.9726,   0.0000,   4.4632,\n",
      "          -0.0000,  -0.0000],\n",
      "        [  0.0000,  12.1098,  -0.0000,  -0.0000,  -0.0000,  -0.0000,  -0.0000,\n",
      "          -0.9910,  -0.0000,  -0.0000,   0.0000,  -0.0000,  -0.0000,  -0.0000,\n",
      "           5.0540,   0.0000],\n",
      "        [  0.0000,  -0.0000,  -0.0000,  -0.0000,   0.0000,  -0.0000,   0.0000,\n",
      "           0.0000,  -0.0000,   0.0000,   0.0000,   0.0000,   0.0000,  -3.6473,\n",
      "          -0.0000,   0.0000],\n",
      "        [ -0.0000,  -0.0000,  -6.9486,   0.0000,   0.0000,  -0.0000,  -0.0000,\n",
      "          -0.5962,  -0.0000,  -1.8244,   0.0000,   0.0000,  -0.0000,   3.2717,\n",
      "           0.0000,  -0.0000],\n",
      "        [  6.0011,   0.0000,   0.0000,   0.0000,  -0.0000,  -0.0000,   0.0000,\n",
      "           0.0000,  -0.0000,   0.0000,  -0.0000,  -0.0000,  10.7402,   0.0000,\n",
      "          -0.0000,  -0.0000],\n",
      "        [  0.0000,   0.0000,  -0.0000,   7.0094,   0.0000,  -0.0000,   0.0000,\n",
      "           0.0000,  -0.0000,   0.0000,   0.0000,  -0.0000,  -0.0000,  -0.0000,\n",
      "           9.7081,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,  -0.0000,   0.0000,  -0.0000,  -0.0000,\n",
      "          -0.0000,   1.4353,  11.2292,  -0.0000,  -0.0000,   0.0000,   0.0000,\n",
      "           2.6918,  -0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,  -1.4768,\n",
      "          -0.0000,  -0.0000,  -0.0000,  -0.5930,  -0.0000,  -0.0000,   0.3721,\n",
      "           0.0000,   0.0000],\n",
      "        [ -0.0000,  -0.0000,   0.0000,  -0.0000,  -2.7923,  -0.6432,   0.0000,\n",
      "          -0.0000,   0.0000,   0.0000,  -0.0000,  -0.0000,  -0.0000,  -0.0000,\n",
      "          -0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,  -0.0000,  -0.0000,  -0.0000,  -0.0000,\n",
      "           0.0000,  -0.0000,   0.0000,  -0.0000,  -0.0000,  -0.0000,   0.0000,\n",
      "           5.3175,   0.0000],\n",
      "        [  0.9173,   0.0000,   0.0000,  -0.0000,   4.6453,   0.0000,   0.0000,\n",
      "           5.3640,  -0.0000,  -0.0000,  -0.0000,   0.0000,  -0.0000,   3.5801,\n",
      "          -0.0000,  -0.0000],\n",
      "        [ -0.0000,  -1.0710,   0.0000,  -0.0000, -10.4627,  -0.0000,   0.0000,\n",
      "          -0.0000,   0.0000,   0.0000,   1.5549,   7.4760,   0.0000,   3.4687,\n",
      "           3.3837,   0.0000],\n",
      "        [  0.0000,  -0.0000,  -0.0000,  -5.4186,   0.0000,  -0.0000,   1.1947,\n",
      "           0.0000,   0.7994,   0.0000,   0.0000,   0.0000,  -0.2842,  -0.0000,\n",
      "           0.0000,  -0.0000],\n",
      "        [  0.0000,  -0.0000,  -4.8720,  -3.6198,   0.0000,   0.0000,  -0.0000,\n",
      "           9.9847,  -0.0000,  -0.0000,  -5.3675,  -1.5301,  -0.0000,   0.0000,\n",
      "          -0.0000,  -3.0830],\n",
      "        [  7.0345,  -0.0000,  -2.3930,   4.0610,   0.0000,  -1.5496,  -3.5121,\n",
      "          -2.0533,  -0.0000,  -0.0000,  -1.4790,  -0.0000,  -0.0000,   0.0000,\n",
      "          -4.6386,  -0.0000],\n",
      "        [  0.0000,  -0.0000,   0.0000,  -0.0000,  -0.0000,  -0.2390,  -0.0000,\n",
      "           0.0000,   0.0000,  -0.0000,   0.0000,   0.0000,  -0.0000,  -0.5461,\n",
      "          -1.6131,  -0.0000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.arange(24, dtype=torch.int32).reshape(3, 8)\n",
    "z = x.view(-1, 3, 4)\n",
    "dropout = torch.nn.Dropout(0.8)\n",
    "input = torch.randn(20, 16)\n",
    "print(dropout(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "mask = (1 - torch.triu(torch.ones(5, 5), diagonal=1)).bool()\n",
    "print(mask.float().masked_fill(mask==0, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 10/10 [00:05<00:00,  1.85it/s, loss=9]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "with tqdm(range(10), position=0, leave=True) as tepoch:\n",
    "  for x in tepoch:\n",
    "    tepoch.set_description(f\"Epoch {x}\")\n",
    "    tepoch.set_postfix(loss=x)\n",
    "    tepoch.update(1)\n",
    "    time.sleep(0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/BoXiaolei/MyTransformer_pytorch\n",
    "from data import *\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# 用来表示一个词的向量长度\n",
    "d_model = 512\n",
    "# FFN的隐藏层神经元个数\n",
    "d_ff = 2048\n",
    "# 分头后的q、k、v词向量长度，依照原文我们都设为64\n",
    "# 原文：queries and kes of dimention d_k,and values of dimension d_v .所以q和k的长度都用d_k来表示\n",
    "d_k = d_v = 64\n",
    "# Encoder Layer 和 Decoder Layer的个数\n",
    "n_layers = 6\n",
    "# 多头注意力中head的个数，原文：we employ h = 8 parallel attention layers, or heads\n",
    "n_heads = 8\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(\n",
    "        self, d_model, dropout=0.1, max_len=5000\n",
    "    ):  # dropout是原文的0.1，max_len原文没找到\n",
    "        \"\"\"max_len是假设的一个句子最多包含5000个token,即max_seq_len\"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        # 开始位置编码部分,先生成一个max_len * d_model 的tensor，即5000 * 512\n",
    "        # 5000是一个句子中最多的token数，512是一个token用多长的向量来表示，5000*512这个矩阵用于表示一个句子的信息\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(\n",
    "            1\n",
    "        )  # pos的shape为[max_len,1],即[5000,1]\n",
    "        # 先把括号内的分式求出来,pos是[5000,1],分母是[256],通过广播机制相乘后是[5000,256]\n",
    "        div_term = pos / pow(\n",
    "            10000.0, torch.arange(0, d_model, 2).float() / d_model\n",
    "        )\n",
    "        # 再取正余弦\n",
    "        pe[:, 0::2] = torch.sin(div_term)\n",
    "        pe[:, 1::2] = torch.cos(div_term)\n",
    "        # 一个句子要做一次pe，一个batch中会有多个句子，所以增加一维来用和输入的一个batch的数据相加时做广播\n",
    "        pe = pe.unsqueeze(0)  # [5000,512] -> [1,5000,512]\n",
    "        # 将pe作为固定参数保存到缓冲区，不会被更新\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"x: [batch_size, seq_len, d_model]\"\"\"\n",
    "        # 5000是我们预定义的最大的seq_len，就是说我们把最多的情况pe都算好了，用的时候用多少就取多少\n",
    "        x = (\n",
    "            x + self.pe[:, : x.size(1), :]\n",
    "        )  # 加的时候应该也广播了，第一维 1 -> batch_size\n",
    "        return self.dropout(\n",
    "            x\n",
    "        )  # return: [batch_size, seq_len, d_model], 和输入的形状相同\n",
    "\n",
    "\n",
    "# 将输入序列中的占位符P的token（就是0） mask掉，用于计算注意力\n",
    "# 返回一个[batch_size, len_q, len_k]大小的布尔张量，True是需要mask掉的位置\n",
    "def get_attn_pad_mask(seq_q, seq_k):\n",
    "    # len_q、len_k其实是q的length和k的length，q和k都是一个序列即一个句子，长度即句子中包含的词的数量\n",
    "    batch_size, len_q = seq_q.size()  # 获取作为q的序列（句子）长度\n",
    "    batch_size, len_k = seq_k.size()  # 获取作为k的序列长度\n",
    "    # seq_k.data.eq(0)返回一个和seq_k等大的布尔张量，seq_k元素等于0的位置为True,否则为False\n",
    "    # 然后扩维以保证后续操作的兼容(广播)\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(\n",
    "        1\n",
    "    )  # pad_attn_mask: [batch_size,1,len_k]\n",
    "    # 要为每一个q提供一份k，所以把第二维度扩展了q次，这样只有最后一列是True，正常来说最后一行也需要是True，但是由于作为padding的token对其他词的注意力不重要，所以可以这样写\n",
    "    res = pad_attn_mask.expand(batch_size, len_q, len_k)\n",
    "    return res  # return: [batch_size, len_q, len_k]\n",
    "    # 返回的是batch_size个 len_q * len_k的矩阵，内容是True和False，第i行第j列表示的是query的第i个词对key的第j个词的注意力是否无意义，若无意义则为True，有意义的为False（即被padding的位置是True）\n",
    "\n",
    "\n",
    "# 用于获取对后续位置的掩码，防止在预测过程中看到未来时刻的输入\n",
    "# 原文：to prevent positions from attending to subsequent positions\n",
    "def get_attn_subsequence_mask(seq):\n",
    "    \"\"\"seq: [batch_size, tgt_len]\"\"\"\n",
    "    # batch_size个 tgt_len * tgt_len的mask矩阵\n",
    "    attn_shape = [seq.size(0), seq.size(1), seq.size(1)]\n",
    "    # np.triu 是生成一个 upper triangular matrix 上三角矩阵，k是相对于主对角线的偏移量\n",
    "    # k=1意为不包含主对角线（从主对角线向上偏移1开始）\n",
    "    subsequence_mask = np.triu(np.ones(attn_shape), k=1)\n",
    "    subsequence_mask = torch.from_numpy(\n",
    "        subsequence_mask\n",
    "    ).byte()  # 因为只有0、1所以用byte节省内存\n",
    "    return subsequence_mask\n",
    "    # return: [batch_size, tgt_len, tgt_len]\n",
    "\n",
    "\n",
    "class ScaledDotProductionAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductionAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        Q: [batch_size, n_heads, len_q, d_k]\n",
    "        K: [batch_size, n_heads, len_k, d_k]\n",
    "        V: [batch_size, n_heads, len_v(=len_k), d_v] 全文两处用到注意力，一处是self attention，另一处是co attention，前者不必说，后者的k和v都是encoder的输出，所以k和v的形状总是相同的\n",
    "        attn_mask: [batch_size, n_heads, seq_len, seq_len]\n",
    "        \"\"\"\n",
    "        # 1) 计算注意力分数QK^T/sqrt(d_k)\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(\n",
    "            d_k\n",
    "        )  # scores: [batch_size, n_heads, len_q, len_k]\n",
    "        # 2)  进行 mask 和 softmax\n",
    "        # mask为True的位置会被设为-1e9\n",
    "        scores.masked_fill_(attn_mask, -1e9)\n",
    "        attn = nn.Softmax(dim=-1)(\n",
    "            scores\n",
    "        )  # attn: [batch_size, n_heads, len_q, len_k]\n",
    "        # 3) 乘V得到最终的加权和\n",
    "        context = torch.matmul(\n",
    "            attn, V\n",
    "        )  # context: [batch_size, n_heads, len_q, d_v]\n",
    "        \"\"\"\n",
    "        得出的context是每个维度(d_1-d_v)都考虑了在当前维度(这一列)当前token对所有token的注意力后更新的新的值，\n",
    "        换言之每个维度d是相互独立的，每个维度考虑自己的所有token的注意力，所以可以理解成1列扩展到多列\n",
    "\n",
    "        返回的context: [batch_size, n_heads, len_q, d_v]本质上还是batch_size个句子，\n",
    "        只不过每个句子中词向量维度512被分成了8个部分，分别由8个头各自看一部分，每个头算的是整个句子(一列)的512/8=64个维度，最后按列拼接起来\n",
    "        \"\"\"\n",
    "        return context  # context: [batch_size, n_heads, len_q, d_v]\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "        self.concat = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, input_Q, input_K, input_V, attn_mask):\n",
    "        \"\"\"\n",
    "        input_Q: [batch_size, len_q, d_model] len_q是作为query的句子的长度，比如enc_inputs（2,5,512）作为输入，那句子长度5就是len_q\n",
    "        input_K: [batch_size, len_k, d_model]\n",
    "        input_K: [batch_size, len_v(len_k), d_model]\n",
    "        attn_mask: [batch_size, seq_len, seq_len]\n",
    "        \"\"\"\n",
    "        residual, batch_size = input_Q, input_Q.size(0)\n",
    "\n",
    "        # 1）linear projection [batch_size, seq_len, d_model] ->  [batch_size, n_heads, seq_len, d_k/d_v]\n",
    "        Q = (\n",
    "            self.W_Q(input_Q)\n",
    "            .view(batch_size, -1, n_heads, d_k)\n",
    "            .transpose(1, 2)\n",
    "        )  # Q: [batch_size, n_heads, len_q, d_k]\n",
    "        K = (\n",
    "            self.W_K(input_K)\n",
    "            .view(batch_size, -1, n_heads, d_k)\n",
    "            .transpose(1, 2)\n",
    "        )  # K: [batch_size, n_heads, len_k, d_k]\n",
    "        V = (\n",
    "            self.W_V(input_V)\n",
    "            .view(batch_size, -1, n_heads, d_v)\n",
    "            .transpose(1, 2)\n",
    "        )  # V: [batch_size, n_heads, len_v(=len_k), d_v]\n",
    "\n",
    "        # 2）计算注意力\n",
    "        # 自我复制n_heads次，为每个头准备一份mask\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(\n",
    "            1, n_heads, 1, 1\n",
    "        )  # attn_mask: [batch_size, n_heads, seq_len, seq_len]\n",
    "        context = ScaledDotProductionAttention()(\n",
    "            Q, K, V, attn_mask\n",
    "        )  # context: [batch_size, n_heads, len_q, d_v]\n",
    "\n",
    "        # 3）concat部分\n",
    "        context = torch.cat(\n",
    "            [context[:, i, :, :] for i in range(context.size(1))], dim=-1\n",
    "        )\n",
    "        output = self.concat(context)  # [batch_size, len_q, d_model]\n",
    "        return nn.LayerNorm(d_model).cuda()(\n",
    "            output + residual\n",
    "        )  # output: [batch_size, len_q, d_model]\n",
    "\n",
    "        \"\"\"        \n",
    "        最后的concat部分，网上的大部分实现都采用的是下面这种方式（也是哈佛NLP团队的写法），但是我发现下面这种方式拼回去会使原来的位置乱序，于是并未采用这种写法，实验效果是相近的\n",
    "        context = context.transpose(1, 2).reshape(batch_size, -1, d_model)\n",
    "        output = self.linear(context)\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "# 这部分代码很简单，对应模型图中的 Feed Forward和 Add & Norm\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        # 就是一个MLP\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff), nn.ReLU(), nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"inputs: [batch_size, seq_len, d_model]\"\"\"\n",
    "        residual = inputs\n",
    "        output = self.fc(inputs)\n",
    "        return nn.LayerNorm(d_model).cuda()(\n",
    "            output + residual\n",
    "        )  # return： [batch_size, seq_len, d_model] 形状不变\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PositionwiseFeedForward()\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        \"\"\"\n",
    "        enc_inputs: [batch_size, src_len, d_model]\n",
    "        enc_self_attn_mask: [batch_size, src_len, src_len]\n",
    "        \"\"\"\n",
    "        # Q、K、V均为 enc_inputs\n",
    "        enc_ouputs = self.enc_self_attn(\n",
    "            enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask\n",
    "        )  # enc_ouputs: [batch_size, src_len, d_model]\n",
    "        enc_ouputs = self.pos_ffn(\n",
    "            enc_ouputs\n",
    "        )  # enc_outputs: [batch_size, src_len, d_model]\n",
    "        return enc_ouputs  # enc_outputs: [batch_size, src_len, d_model]\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        # 直接调的现成接口完成词向量的编码，输入是类别数和每一个类别要映射成的向量长度\n",
    "        self.src_emb = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.pos_emb = PositionalEncoding(d_model)\n",
    "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, enc_inputs):\n",
    "        \"\"\"enc_inputs: [batch_size, src_len]\"\"\"\n",
    "        enc_outputs = self.src_emb(\n",
    "            enc_inputs\n",
    "        )  # [batch_size, src_len] -> [batch_size, src_len, d_model]\n",
    "        enc_outputs = self.pos_emb(\n",
    "            enc_outputs\n",
    "        )  # enc_outputs: [batch_size, src_len, d_model]\n",
    "        # Encoder中是self attention，所以传入的Q、K都是enc_inputs\n",
    "        enc_self_attn_mask = get_attn_pad_mask(\n",
    "            enc_inputs, enc_inputs\n",
    "        )  # enc_self_attn_mask: [batch_size, src_len, src_len]\n",
    "        for layer in self.layers:\n",
    "            enc_outputs = layer(enc_outputs, enc_self_attn_mask)\n",
    "        return enc_outputs  # enc_outputs: [batch_size, src_len, d_model]\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.dec_self_attn = MultiHeadAttention()\n",
    "        self.dec_enc_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PositionwiseFeedForward()\n",
    "\n",
    "    def forward(\n",
    "        self, dec_inputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask\n",
    "    ):\n",
    "        \"\"\"\n",
    "        dec_inputs: [batch_size, tgt_len, d_model]\n",
    "        enc_outputs: [batch_size, src_len, d_model]\n",
    "        dec_self_attn_mask: [batch_size, tgt_len, tgt_len]\n",
    "        dec_enc_attn_mask: [batch_size, tgt_len, src_len] 前者是Q后者是K\n",
    "        \"\"\"\n",
    "        dec_outputs = self.dec_self_attn(\n",
    "            dec_inputs, dec_inputs, dec_inputs, dec_self_attn_mask\n",
    "        )\n",
    "        dec_outputs = self.dec_enc_attn(\n",
    "            dec_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask\n",
    "        )\n",
    "        dec_outputs = self.pos_ffn(dec_outputs)\n",
    "\n",
    "        return dec_outputs  # dec_outputs: [batch_size, tgt_len, d_model]\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.pos_emb = PositionalEncoding(d_model)\n",
    "        self.layers = nn.ModuleList([DecoderLayer() for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, dec_inputs, enc_inputs, enc_outputs):\n",
    "        \"\"\"\n",
    "        这三个参数对应的不是Q、K、V，dec_inputs是Q，enc_outputs是K和V，enc_inputs是用来计算padding mask的\n",
    "        dec_inputs: [batch_size, tgt_len]\n",
    "        enc_inpus: [batch_size, src_len]\n",
    "        enc_outputs: [batch_size, src_len, d_model]\n",
    "        \"\"\"\n",
    "        dec_outputs = self.tgt_emb(dec_inputs)\n",
    "        dec_outputs = self.pos_emb(dec_outputs).cuda()\n",
    "        dec_self_attn_pad_mask = get_attn_pad_mask(\n",
    "            dec_inputs, dec_inputs\n",
    "        ).cuda()\n",
    "        dec_self_attn_subsequence_mask = get_attn_subsequence_mask(\n",
    "            dec_inputs\n",
    "        ).cuda()\n",
    "        # 将两个mask叠加，布尔值可以视为0和1，和大于0的位置是需要被mask掉的，赋为True，和为0的位置是有意义的为False\n",
    "        dec_self_attn_mask = torch.gt(\n",
    "            (dec_self_attn_pad_mask + dec_self_attn_subsequence_mask), 0\n",
    "        ).cuda()\n",
    "        # 这是co-attention部分，为啥传入的是enc_inputs而不是enc_outputs呢\n",
    "        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            dec_outputs = layer(\n",
    "                dec_outputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask\n",
    "            )\n",
    "\n",
    "        return dec_outputs  # dec_outputs: [batch_size, tgt_len, d_model]\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder().cuda()\n",
    "        self.decoder = Decoder().cuda()\n",
    "        self.projection = nn.Linear(d_model, tgt_vocab_size).cuda()\n",
    "\n",
    "    def forward(self, enc_inputs, dec_inputs):\n",
    "        \"\"\"\n",
    "        enc_inputs: [batch_size, src_len]\n",
    "        dec_inputs: [batch_size, tgt_len]\n",
    "        \"\"\"\n",
    "        enc_outputs = self.encoder(enc_inputs)\n",
    "        dec_outputs = self.decoder(dec_inputs, enc_inputs, enc_outputs)\n",
    "        dec_logits = self.projection(\n",
    "            dec_outputs\n",
    "        )  # dec_logits: [batch_size, tgt_len, tgt_vocab_size]\n",
    "\n",
    "        # 解散batch，一个batch中有batch_size个句子，每个句子有tgt_len个词（即tgt_len行），现在让他们按行依次排布，如前tgt_len行是第一个句子的每个词的预测概率，再往下tgt_len行是第二个句子的，一直到batch_size * tgt_len行\n",
    "        return dec_logits.view(\n",
    "            -1, dec_logits.size(-1)\n",
    "        )  #  [batch_size * tgt_len, tgt_vocab_size]\n",
    "        \"\"\"最后变形的原因是：nn.CrossEntropyLoss接收的输入的第二个维度必须是类别\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 2., 3.],\n",
      "          [1., 2., 3.]],\n",
      "\n",
      "         [[1., 2., 3.],\n",
      "          [1., 2., 3.]],\n",
      "\n",
      "         [[1., 2., 3.],\n",
      "          [1., 2., 3.]],\n",
      "\n",
      "         [[1., 2., 3.],\n",
      "          [1., 2., 3.]]],\n",
      "\n",
      "\n",
      "        [[[4., 5., 6.],\n",
      "          [4., 5., 6.]],\n",
      "\n",
      "         [[4., 5., 6.],\n",
      "          [4., 5., 6.]],\n",
      "\n",
      "         [[4., 5., 6.],\n",
      "          [4., 5., 6.]],\n",
      "\n",
      "         [[4., 5., 6.],\n",
      "          [4., 5., 6.]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "t1 = torch.arange(1,7).reshape((2, 1, 1, 3))\n",
    "t2 = torch.zeros(3, 16).reshape(2, 4, 2, 3)\n",
    "print(t1 + t2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
