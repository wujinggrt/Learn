{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * vocab_size=128\n",
      " * model_dim=128\n",
      " * dropout=0.1\n",
      " * n_encoder_layers=1\n",
      " * n_decoder_layers=1\n",
      " * n_heads=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 196/196 [00:12<00:00, 15.80it/s, accuracy=99.8, loss=0.00898]\n",
      "100%|██████████| 40/40 [00:01<00:00, 24.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 1.521,Train acc: 0.660, Val loss: 0.001, Val acc: 1.000 Epoch time = 13.639s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 196/196 [00:13<00:00, 14.25it/s, accuracy=99.8, loss=0.00677]\n",
      "100%|██████████| 40/40 [00:03<00:00, 11.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train loss: 0.004,Train acc: 0.999, Val loss: 0.001, Val acc: 1.000 Epoch time = 15.075s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 196/196 [00:11<00:00, 16.53it/s, accuracy=100, loss=0.000866]\n",
      "100%|██████████| 40/40 [00:01<00:00, 36.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train loss: 0.004,Train acc: 0.999, Val loss: 0.002, Val acc: 1.000 Epoch time = 13.213s\n"
     ]
    }
   ],
   "source": [
    "# https://zhuanlan.zhihu.com/p/690355021\n",
    "import math\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.ERROR)\n",
    "formatter = logging.Formatter(\n",
    "    \"===\"\n",
    "    \"[%(asctime)s - %(levelname)s - %(name)s - %(funcName)s:%(lineno)d]\"\n",
    "    \"===\\n\"\n",
    "    \"%(message)s\\n\"\n",
    ")\n",
    "# 对logger设置输出格式\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim=256, num_heads=4):\n",
    "        \"\"\"\n",
    "        hidden_dim: 输入的维度\n",
    "        num_heads: 输入分成的注意头数量\n",
    "\n",
    "        learning parameter matrices由Linear的layer维护，并且将多个heads放入\n",
    "        一个weight tensor， 使得我们可以方便地更新梯度。\n",
    "        各个head的对应Q,K,V矩阵不一定为方阵，shape为(d_k,d).\n",
    "        d_k=hidden_dim/num_dim，d=hidden_dim。需要注意在Wa中，Q矩阵是如何分布的?\n",
    "        先说答案，在Wq.weight中，每d_k行组成的矩阵为每个head的learning parameter mat.\n",
    "        Wk和Wv也有类似的结论。\n",
    "\n",
    "        我们记每个head的Q和K的矩阵形状为(d_k,d),那么第i个head的Q中，\n",
    "        应该是Wq.weight[(i*d_k):((i+1)*d_k),d]。\n",
    "        注意，nn.Linear.weight的shape为(out_features, in_features)，所以其forward\n",
    "        操作为X @ weight.T。得到结果中，每d_k列为一个head的映射结果。\n",
    "        对应到学习参数矩阵，weight[]中第一维(行)用d_k索引，第二维(列)用d索引。\n",
    "        即weight.T = multi_head_q，而mulit_head_q的shape为(d_k * num_heads, d)\n",
    "\n",
    "        最开始传入q给Wq处理 (传入参数很大可能是q=k=v)。q的size为\n",
    "        (batch_size, length, hidden_num),length即x向量的数量。\n",
    "        执行映射过程为q = self.Wq(q)\n",
    "        即是计算从输入映射后得到的结果，进一步用于计算attention weight。\n",
    "        最后一维作为单个输入的向量，记为x，交由Wq执行映射。对于每个输入向量x，我们需要\n",
    "        得到num_heads个向量。 最后得到的结果是(batch_size, length, num_heads * d_k)\n",
    "        最后一维是按照head堆叠起来的，每个有hidden_num个分量的向量，就是一个\n",
    "        num_heads个d_k维的向量。也就是将结果转换成了d_w维度。\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        assert (\n",
    "            hidden_dim % num_heads == 0\n",
    "        ), \"hidden_dim must be the integer times of num_heads\"\n",
    "        self.Wq = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.Wk = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.Wv = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        # multi-head attention最后一步，在num_heads的结果输出后，将这些向量concat，\n",
    "        # 并输入此fc\n",
    "        self.Wo = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "\n",
    "    def _check_scaled_dot_product_attention_inputs(self, x):\n",
    "        \"\"\"\n",
    "        check scaled dot-product attention inputs\n",
    "\n",
    "        要求x是 (B, H, len, d_k)的\n",
    "        \"\"\"\n",
    "        assert x.size(1) == self.num_heads, (\n",
    "            f\"expects that x has shape as:\"\n",
    "            f\" ({-1, self.num_heads, -1, self.hidden_dim // self.num_heads}),\"\n",
    "            f\"but get {x.size()}\"\n",
    "        )\n",
    "        # assert d_k is ok.\n",
    "        assert x.size(3) == self.hidden_dim // self.num_heads\n",
    "\n",
    "    def _scaled_dot_product_attention(\n",
    "        self, query, key, value, attention_mask=None, key_padding_mask=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        query: tensor, shape is\n",
    "        (batch_size, num_heads, query_sequence_length, hidden_dim//num_heads)\n",
    "        key: tensor, shape is\n",
    "        (batch_size, num_heads, key_sequence_length, hidden_dim // num_heads)\n",
    "        value: tensor, shape is\n",
    "        (batch_size, num_heads, value_sequence_length, hidden_dim//num_heads)\n",
    "        attention_mask: tensor, shape is\n",
    "        (query_sequence_length, key_sequence_length)\n",
    "        key_padding_mask: tensor, shape is\n",
    "        (sequence_length, key_sequence_length)\n",
    "\n",
    "        query最开始是(batch_size, query_sequence_length, hidden_dim)的,w，\n",
    "        经过split_into_heads后的结果。\n",
    "        \"\"\"\n",
    "        self._check_scaled_dot_product_attention_inputs(query)\n",
    "        self._check_scaled_dot_product_attention_inputs(key)\n",
    "        self._check_scaled_dot_product_attention_inputs(value)\n",
    "\n",
    "        d_k = key.size(-1)\n",
    "        # tgt_len and src_len is sequence_length，也就是当前batch中字母的个数\n",
    "        tgt_len, src_len = query.size(-2), key.size(-2)\n",
    "\n",
    "        # logits = (B, H, tgt_len, E) * (B, H, E, src_len) =\n",
    "        # (B, H, tgt_len, src_len)\n",
    "        logits = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "        # 注意力遮罩\n",
    "        if attention_mask is not None:\n",
    "            if attention_mask.dim() == 2:\n",
    "                # 保证能够正常作用于q和k\n",
    "                assert attention_mask.size() == (tgt_len, src_len)\n",
    "                # reshape为 (1, query_sequence_length, key_sequence_length)\n",
    "                # 方便于logits广播\n",
    "                # attention_mask = attention_mask.unsqueeze(0)\n",
    "                logger.info(\n",
    "                    f\"attention mask size is {attention_mask.size()}\\n\"\n",
    "                    f\"logits size is {logits.size()}\"\n",
    "                )\n",
    "                logits = logits + attention_mask\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"attention_mask.size() is invalid: {attention_mask.size()}\"\n",
    "                )\n",
    "\n",
    "        if key_padding_mask is not None:\n",
    "            if key_padding_mask.dim() == 2:\n",
    "                assert key_padding_mask.size() == (\n",
    "                    query.size(0),\n",
    "                    key.size(2),\n",
    "                ), (\n",
    "                    f\"key_padding_mask.size() is invalid:\"\n",
    "                    f\"{key_padding_mask.size()}\\nbatch size is {query.size(0)}, \"\n",
    "                    f\"sequence size is {key.size(2)}\"\n",
    "                )\n",
    "                \"\"\"\n",
    "                在批次大小，注意力头上广播。有logits的维度是\n",
    "                (batch_size, num_heads, query_sequence_length, key_sequence_length)\n",
    "                key_padding_mask的维度是(batch_size, key_sequence_length)\n",
    "                广播后，key_padding_mask的维度是\n",
    "                (batch_size, num_heads, query_sequence_length, key_sequence_length)\n",
    "                这样，broadcast机制就可以直接作用到logits的最后两个维度上。\n",
    "                \"\"\"\n",
    "                key_padding_mask = key_padding_mask.unsqueeze(1).unsqueeze(2)\n",
    "                # 直接使用广播机制与logits的最后两个维度相加\n",
    "                logger.info(\n",
    "                    f\"key_padding_mask.size() is:{key_padding_mask.size()}\\n\"\n",
    "                    f\"logits.size() is:{logits.size()}\"\n",
    "                )\n",
    "                logits = logits + key_padding_mask\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"key_padding_mask.size() is invalid: {key_padding_mask.size()}\"\n",
    "                )\n",
    "\n",
    "        attention = torch.softmax(logits, dim=-1)\n",
    "        # (batch_size, num_heads, sequence_length, hidden_dim)\n",
    "        output = torch.matmul(attention, value)\n",
    "        return output, attention\n",
    "\n",
    "    def _split_into_heads(self, x):\n",
    "        batch_size, seq_length, hidden_dim = x.size()\n",
    "        d_k = hidden_dim // self.num_heads\n",
    "        # 拆解了最后一维，方便送到每个head的learning parameter matrices\n",
    "        x = x.view(batch_size, seq_length, self.num_heads, d_k)\n",
    "        # 最终返回(batch_size, num_heads, seq_length, hidden_dim // num_heads)\n",
    "        return x.transpose(1, 2)\n",
    "\n",
    "    # 上一个方法的逆操作\n",
    "    def _combine_heads(self, x):\n",
    "        batch_size, num_heads, seq_length, head_hidden_dim = x.size()\n",
    "        return (\n",
    "            x.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(batch_size, seq_length, num_heads * head_hidden_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, q, k, v, attention_mask=None, key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        q: tensor, shape is (batch_size, query_sequence_length, hidden_dim)\n",
    "        k: tensor, shape is (batch_size, key_sequence_length, hidden_dim)\n",
    "        v: tensor, shape is (batch_size, value_sequence_length, hidden_dim)\n",
    "        attention_mask: tensor, shape is\n",
    "        (query_sequence_length, key_sequence_length)\n",
    "        key_padding_mask: tensor, shape is (sequence_length, key_sequence_length)\n",
    "        \"\"\"\n",
    "        q = self.Wq(q)\n",
    "        k = self.Wk(k)\n",
    "        v = self.Wv(v)\n",
    "\n",
    "        q = self._split_into_heads(q)\n",
    "        k = self._split_into_heads(k)\n",
    "        v = self._split_into_heads(v)\n",
    "\n",
    "        attention_values, attention_weights = (\n",
    "            self._scaled_dot_product_attention(\n",
    "                q, k, v, attention_mask, key_padding_mask\n",
    "            )\n",
    "        )\n",
    "        grouped = self._combine_heads(attention_values)\n",
    "        output = self.Wo(grouped)\n",
    "\n",
    "        self.attention_weights = attention_weights\n",
    "        return output\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        # shape: (max_len, 1)\n",
    "        positions = torch.arange(max_len, dtype=torch.float).unsqueeze(1)\n",
    "        # e^((2k/d_model) * -ln(10000)) = (e^(-ln(10000)))^(2k/d_model)\n",
    "        # = (1/10000)^(2k/d_model)\n",
    "        # 省略了求幂的操作，求对数更高效\n",
    "        division_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10_000) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(positions / division_term)\n",
    "        pe[:, 1::2] = torch.cos(positions / division_term)\n",
    "        # 因为操作都会用batch，所以unsqueeze\n",
    "        # shape: (1, max_len, d_model)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Tensor, shape is [batch_size, sequence_length, embedding_dim]\n",
    "        \"\"\"\n",
    "        # 通过广播，每个batch，都会在pe[0, :x.size(1), :]处加上对应的结果。\n",
    "        x = x + self.pe[:, : x.size(1), :]\n",
    "        return x\n",
    "\n",
    "\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, n_dim: int, dropout: float, n_heads: int) -> None:\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(hidden_dim=n_dim, num_heads=n_heads)\n",
    "        self.norm1 = nn.LayerNorm(n_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.feed_forward = PositionWiseFeedForward(n_dim, n_dim)\n",
    "        self.norm2 = nn.LayerNorm(n_dim)\n",
    "\n",
    "    def forward(self, x, src_padding_mask=None):\n",
    "        assert x.ndim == 3, f\"Expected input to be 3-dim, got {x.ndim}\"\n",
    "        attention_output = self.mha(x, x, x, key_padding_mask=src_padding_mask)\n",
    "        attention_output = self.norm1(attention_output)\n",
    "        # Resudual connection\n",
    "        x = x + self.dropout(attention_output)\n",
    "\n",
    "        x = self.feed_forward(x)\n",
    "        # Resudual connection\n",
    "        x = x + self.norm2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        n_dim: int,\n",
    "        dropout: float,\n",
    "        n_encoder_blocks: int,\n",
    "        n_heads: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.n_dim = n_dim\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size, embedding_dim=n_dim\n",
    "        )\n",
    "        self.positional_encoding = PositionalEncoding(n_dim, dropout=dropout)\n",
    "        self.encoder_blocks = nn.ModuleList(\n",
    "            [\n",
    "                EncoderBlock(n_dim, dropout, n_heads)\n",
    "                for _ in range(n_encoder_blocks)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, padding_mask=None):\n",
    "        # Transformer论文中，指出需要对embedding vector乘以sqrt(d_model)\n",
    "        x = self.embedding(x) * math.sqrt(self.n_dim)\n",
    "        x = self.positional_encoding(x)\n",
    "        for block in self.encoder_blocks:\n",
    "            x = block(x, padding_mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, n_dim: int, dropout: float, n_heads: int) -> None:\n",
    "        super().__init__()\n",
    "        # The first multi-head attention is to be masked to prevent seeing the future infomations.\n",
    "        self.self_attention = MultiHeadAttention(\n",
    "            hidden_dim=n_dim, num_heads=n_heads\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(n_dim)\n",
    "        # The second multi-head attention takes the outputs from encoder\n",
    "        self.cross_attention = MultiHeadAttention(\n",
    "            hidden_dim=n_dim, num_heads=n_heads\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(n_dim)\n",
    "\n",
    "        self.feed_forward = PositionWiseFeedForward(n_dim, n_dim)\n",
    "        self.norm3 = nn.LayerNorm(n_dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tgt,\n",
    "        memory,\n",
    "        tgt_mask=None,\n",
    "        tgt_padding_mask=None,\n",
    "        memory_padding_mask=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        memory: come from encoder\n",
    "        \"\"\"\n",
    "        masked_attention_output = self.self_attention(\n",
    "            q=tgt,\n",
    "            k=tgt,\n",
    "            v=tgt,\n",
    "            attention_mask=tgt_mask,\n",
    "            key_padding_mask=tgt_padding_mask,\n",
    "        )\n",
    "        # resudual connection\n",
    "        x1 = tgt + self.norm1(masked_attention_output)\n",
    "\n",
    "        cross_attention_output = self.cross_attention(\n",
    "            q=x1,\n",
    "            k=memory,\n",
    "            v=memory,\n",
    "            attention_mask=None,\n",
    "            key_padding_mask=memory_padding_mask,\n",
    "        )\n",
    "        # resudual connection\n",
    "        x2 = x1 + self.norm2(cross_attention_output)\n",
    "        ff_output = self.feed_forward(x2)\n",
    "        # resudual connection\n",
    "        output = x2 + self.norm3(ff_output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        n_dim: int,\n",
    "        dropout: float,\n",
    "        n_decoder_blocks: int,\n",
    "        n_heads: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size, embedding_dim=n_dim\n",
    "        )\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            d_model=n_dim, dropout=dropout\n",
    "        )\n",
    "        self.decoder_blocks = nn.ModuleList(\n",
    "            [\n",
    "                DecoderBlock(n_dim=n_dim, dropout=dropout, n_heads=n_heads)\n",
    "                for _ in range(n_decoder_blocks)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tgt,\n",
    "        memory,\n",
    "        tgt_mask=None,\n",
    "        tgt_padding_mask=None,\n",
    "        memory_padding_mask=None,\n",
    "    ):\n",
    "        x = self.embedding(tgt)\n",
    "        x = self.positional_encoding(x)\n",
    "        for block in self.decoder_blocks:\n",
    "            x = block(\n",
    "                tgt=x,\n",
    "                memory=memory,\n",
    "                tgt_mask=tgt_mask,\n",
    "                tgt_padding_mask=tgt_padding_mask,\n",
    "                memory_padding_mask=memory_padding_mask,\n",
    "            )\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        for k, v in kwargs.items():\n",
    "            print(f\" * {k}={v}\")\n",
    "\n",
    "        self.vocab_size = kwargs.get(\"vocab_size\")\n",
    "        self.model_dim = kwargs.get(\"model_dim\")\n",
    "        self.dropout = kwargs.get(\"dropout\")\n",
    "        self.n_encoder_layers = kwargs.get(\"n_encoder_layers\")\n",
    "        self.n_decoder_layers = kwargs.get(\"n_decoder_layers\")\n",
    "        self.n_heads = kwargs.get(\"n_heads\")\n",
    "        self.batch_size = kwargs.get(\"batch_size\")\n",
    "        self.PAD_IDX = kwargs.get(\"pad_idx\", 0)\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            self.vocab_size,\n",
    "            self.model_dim,\n",
    "            self.dropout,\n",
    "            self.n_encoder_layers,\n",
    "            self.n_heads,\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            self.vocab_size,\n",
    "            self.model_dim,\n",
    "            self.dropout,\n",
    "            self.n_decoder_layers,\n",
    "            self.n_heads,\n",
    "        )\n",
    "        # 最后输出(B,S,vocab_size)，由于没有激活层，所以是logits\n",
    "        self.fc = nn.Linear(self.model_dim, self.vocab_size)\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_square_subsequent_mask(size: int):\n",
    "        \"\"\"\n",
    "        生成一个大小为 (size, size) 的上三角形状的掩码。来自 PyTorch 文档。\n",
    "\n",
    "        return:\n",
    "            主对角线及下三角为0.0，其余为-inf\n",
    "        \"\"\"\n",
    "        mask = (1 - torch.triu(torch.ones(size, size), diagonal=1)).bool()\n",
    "        mask = (\n",
    "            mask.float()\n",
    "            .masked_fill(mask == 0, float(\"-inf\"))\n",
    "            .masked_fill(mask == 1, 0.0)\n",
    "        )\n",
    "        return mask\n",
    "\n",
    "    def encode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        input:\n",
    "            x: (B, S), B是batch size, S是序列长度，比如单词padding之后的长度。\n",
    "            在Embedded层处理后，S会被处理，每个字符会被Embedded层映射到常委model_dim\n",
    "            的rank-1 tensor，最后映射得到(B, S, model_dim)\n",
    "        output:\n",
    "            (B, S, model_dim)\n",
    "        \"\"\"\n",
    "        # 在encoder中，mask掉pad_idx对应的位置\n",
    "        mask = (x == self.PAD_IDX).float()\n",
    "        # 将x的填充部分设置为-inf\n",
    "        # encoder_padding_mask与x有相同size，shape为(B, S)\n",
    "        # 此时还没有经过Embeded层处理。比如：\n",
    "        encoder_padding_mask = mask.masked_fill(mask == 1, float(\"-inf\"))\n",
    "        logger.info(\n",
    "            f\"encoder start, encoder padding mask size is {encoder_padding_mask.size()}\"\n",
    "        )\n",
    "        # (B, S, E)\n",
    "        # 此mask作为MHA的scaled dot product attention中的key_padding_mask参数。\n",
    "        encoder_output = self.encoder(x, padding_mask=encoder_padding_mask)\n",
    "        logger.info(\n",
    "            f\"encoder output has size {encoder_output.size()}\\n\"\n",
    "            f\"memory mask size is {encoder_padding_mask.size()}\"\n",
    "        )\n",
    "        return encoder_output, encoder_padding_mask\n",
    "\n",
    "    def decode(\n",
    "        self, tgt: torch.Tensor, memory: torch.Tensor, memory_padding_mask=None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        B for Batch size, S for source sequence length, L for target sequence\n",
    "        length, E for embedding dimension.\n",
    "\n",
    "        intput:\n",
    "            encoded_x: (B, S, E)\n",
    "            y: (B, L), 元素在(0, C)区间内\n",
    "        output:\n",
    "            (B, L, C)\n",
    "        \"\"\"\n",
    "\n",
    "        mask = (tgt == self.PAD_IDX).float()\n",
    "        tgt_padding_mask = mask.masked_fill(mask == 1, float(\"-inf\"))\n",
    "        logger.info(\n",
    "            f\"decoder start, tgt padding mask size is {tgt_padding_mask.size()}\"\n",
    "        )\n",
    "\n",
    "        decoder_output = self.decoder(\n",
    "            tgt=tgt,\n",
    "            memory=memory,\n",
    "            tgt_mask=self.generate_square_subsequent_mask(tgt.size(1)),\n",
    "            tgt_padding_mask=tgt_padding_mask,\n",
    "            memory_padding_mask=memory_padding_mask,\n",
    "        )\n",
    "        logger.info(\n",
    "            f\"decoder output (before fc) has size {decoder_output.size()}\"\n",
    "        )\n",
    "        output = self.fc(decoder_output)\n",
    "        return output\n",
    "\n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        input:\n",
    "            x: (B, Sx), Sx是单词的padding之后的长度。\n",
    "            y: (B, Sy), Sy是单词的padding之后的长度。\n",
    "        output:\n",
    "            (B, L, C)\n",
    "        \"\"\"\n",
    "\n",
    "        # Encoder output a tensor with shape (B, S, E)\n",
    "        encoder_output, encoder_padding_mask = self.encode(x)\n",
    "        # Decoder output a tensor with shape (B, L, C)\n",
    "        decoder_output = self.decode(\n",
    "            tgt=y,\n",
    "            memory=encoder_output,\n",
    "            memory_padding_mask=encoder_padding_mask,\n",
    "        )\n",
    "        return decoder_output\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        start_of_sentence_index: int = 1,\n",
    "        end_of_sentence_index: int = 2,\n",
    "        max_length: int = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        推断时候使用此方法。逐个标记。\n",
    "        input:\n",
    "            x: a single str Tensor\n",
    "        output:\n",
    "            (B, L, C), the last is logits\n",
    "        \"\"\"\n",
    "        # 标记句子开头与结尾\n",
    "        x = torch.cat(\n",
    "            [\n",
    "                torch.tensor([start_of_sentence_index]),\n",
    "                x,\n",
    "                torch.tensor([end_of_sentence_index]),\n",
    "            ]\n",
    "        ).unsqueeze(0)\n",
    "        encoder_output, mask = self.transformer.encoder(x)  # (B, S, E)\n",
    "\n",
    "        if not max_length:\n",
    "            # 默认使用句子长度作为最大长度\n",
    "            # 这也是我们样例中，反转句子的要求，与输入同长。\n",
    "            max_length = x.size(1)\n",
    "\n",
    "        outputs = (\n",
    "            torch.ones((x.size()[0], max_length)).type_as(x).long()\n",
    "            * start_of_sentence_index\n",
    "        )\n",
    "        for step in range(1, max_length):\n",
    "            y = outputs[:, :step]\n",
    "            probs = self.transformer.decode(y, encoder_output)\n",
    "            output = torch.argmax(probs, dim=-1)\n",
    "            print(f\"Knowing {y} we output {output[:, -1]}\")\n",
    "            if output[:, -1].detach().numpy() in (\n",
    "                end_of_sentence_index,\n",
    "                start_of_sentence_index,\n",
    "            ):\n",
    "                break\n",
    "            outputs[:, step] = output[:, -1]\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# 定义小型数据集，用于反转单词，比如\"helloworld\"->\"dlrowolleh\"\n",
    "PAD_IDX = 0\n",
    "SOS_IDX = 1\n",
    "EOS_IDX = 2\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "def generate_random_string():\n",
    "    \"\"\"\n",
    "    随机生成10-20长度的字符串\n",
    "    \"\"\"\n",
    "    len = np.random.randint(10, 20)\n",
    "    # ord(a) == 97\n",
    "    return \"\".join([chr(x) for x in np.random.randint(97, 97 + 26, len)])\n",
    "\n",
    "\n",
    "class ReverseDataset(Dataset):\n",
    "    def __init__(self, n_samples, pad_idx, sos_idx, eos_idx):\n",
    "        super().__init__()\n",
    "        self.pad_idx = pad_idx\n",
    "        self.sos_idx = sos_idx\n",
    "        self.eos_idx = eos_idx\n",
    "        self.values = [generate_random_string() for _ in range(n_samples)]\n",
    "        # reversed values, as label to train.\n",
    "        self.labels = [x[::-1] for x in self.values]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.values)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 返回对应value和label，得到单词和单词的反转\n",
    "        return self.text_transform(\n",
    "            self.values[index].rstrip(\"\\n\")\n",
    "        ), self.text_transform(self.labels[index].rstrip(\"\\n\"))\n",
    "\n",
    "    def text_transform(self, x):\n",
    "        # +3作为offset，是因为pad_idx=0,sos_idx=1,eos_idx=2，a需要从3开始\n",
    "        return torch.tensor(\n",
    "            [self.sos_idx] + [ord(z) - 97 + 3 for z in x] + [self.eos_idx]\n",
    "        )\n",
    "\n",
    "\n",
    "def train(model, optimizer, loader, loss_fn, epoch):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    acc = 0\n",
    "    history_loss = []\n",
    "    history_acc = []\n",
    "\n",
    "    with tqdm(loader, position=0, leave=True) as tepoch:\n",
    "        for x, y in tepoch:\n",
    "            \"\"\"\n",
    "            此时x, y都是一个Batch中的训练数据，有着相同对齐长度，\n",
    "            相同的长度，经过了PAD_IDX填充。\n",
    "            \"\"\"\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "            optimizer.zero_grad()\n",
    "            # 在NLP中，给decoder的训练输入需要SOS_IDX开头，不需要EOS_IDX结尾。\n",
    "            # 但是label数据不需要SOS_IDX开头，需要EOS_IDX结尾。\n",
    "            # encoder中的是输入，decoder是label\n",
    "            train_y = y[:, :-1]\n",
    "            label_y = y[:, 1:]\n",
    "            logger.info(\n",
    "                f\"start model, x size is {x.size()}, train_y size is {train_y.size()}\"\n",
    "            )\n",
    "            # logits是(B, T, vocab_size)\n",
    "            # logits = model(x, train_y)\n",
    "            logits = model(x, train_y)\n",
    "            logger.info(f\"get logits with size {logits.size()}\")\n",
    "            loss = loss_fn(\n",
    "                # 返回视图，Size为二维的(B*T, d_model)，以此作比较\n",
    "                logits.contiguous().view(-1, model.vocab_size),\n",
    "                # 展成一维\n",
    "                label_y.contiguous().view(-1),\n",
    "            )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            logger.info(\"loss steped\")\n",
    "            losses += loss\n",
    "\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            logger.info(f\"preds size is {preds.size()}\")\n",
    "            tmp = label_y != PAD_IDX\n",
    "            logger.info(f\"y[:, 1:] != PAD_IDX size is {tmp.size()}\")\n",
    "            masked_pred = preds * (y[:, 1:] != PAD_IDX)\n",
    "            accuracy = (masked_pred == label_y).float().mean()\n",
    "            acc += accuracy.item()\n",
    "\n",
    "            history_loss.append(loss.item())\n",
    "            history_acc.append(accuracy.item())\n",
    "            tepoch.set_postfix(\n",
    "                loss=loss.item(), accuracy=100.0 * accuracy.item()\n",
    "            )\n",
    "    return (\n",
    "        losses / len(list(loader)),\n",
    "        acc / len(list(loader)),\n",
    "        history_loss,\n",
    "        history_acc,\n",
    "    )\n",
    "\n",
    "\n",
    "def evaluate(model, loader, loss_fn):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "    acc = 0\n",
    "    history_loss = []\n",
    "    history_acc = []\n",
    "\n",
    "    for x, y in tqdm(loader, position=0, leave=True):\n",
    "        logits = model(x, y[:, :-1])\n",
    "        loss = loss_fn(\n",
    "            logits.contiguous().view(-1, model.vocab_size),\n",
    "            y[:, 1:].contiguous().view(-1),\n",
    "        )\n",
    "        losses += loss\n",
    "\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        masked_pred = preds * (y[:, 1:] != PAD_IDX)\n",
    "        accuracy = (masked_pred == y[:, 1:]).float().mean()\n",
    "        acc += accuracy.item()\n",
    "\n",
    "        history_loss.append(loss.item())\n",
    "        history_acc.append(accuracy.item())\n",
    "    return (\n",
    "        losses / len(list(loader)),\n",
    "        acc / len(list(loader)),\n",
    "        history_loss,\n",
    "        history_acc,\n",
    "    )\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    此函数使用PAD_IDX填充输入，使得批次\n",
    "    \"\"\"\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(src_sample)\n",
    "        tgt_batch.append(tgt_sample)\n",
    "    src_batch = pad_sequence(\n",
    "        src_batch, padding_value=PAD_IDX, batch_first=True\n",
    "    )\n",
    "    tgt_batch = pad_sequence(\n",
    "        tgt_batch, padding_value=PAD_IDX, batch_first=True\n",
    "    )\n",
    "    return src_batch, tgt_batch\n",
    "\n",
    "\n",
    "# hyperparameters\n",
    "args = {\n",
    "    # 我们的示例虽然是[a,z]，[97,97+25]，但是已经对齐到了[3,3+25]\n",
    "    # PAD_IDX, SOS_IDX, EOS_IDX分别是0，1，2\n",
    "    \"vocab_size\": 128,\n",
    "    \"model_dim\": 128,\n",
    "    \"dropout\": 0.1,\n",
    "    # 首先使用一层来查看结果\n",
    "    \"n_encoder_layers\": 1,\n",
    "    \"n_decoder_layers\": 1,\n",
    "    \"n_heads\": 4,\n",
    "}\n",
    "\n",
    "model = Transformer(**args)\n",
    "\n",
    "# 实例化数据\n",
    "train_iter = ReverseDataset(\n",
    "    50_000, pad_idx=PAD_IDX, sos_idx=SOS_IDX, eos_idx=EOS_IDX\n",
    ")\n",
    "eval_iter = ReverseDataset(\n",
    "    10_000, pad_idx=PAD_IDX, sos_idx=SOS_IDX, eos_idx=EOS_IDX\n",
    ")\n",
    "dataloader_train = DataLoader(\n",
    "    train_iter, batch_size=256, collate_fn=collate_fn\n",
    ")\n",
    "dataloader_eval = DataLoader(eval_iter, batch_size=256, collate_fn=collate_fn)\n",
    "\n",
    "# 在调试期间，我们确保源和目标确实被反转了\n",
    "# s, t = next(iter(dataloader_train))\n",
    "\n",
    "# 初始化参数模型\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "# 忽略填充，不计算其loss，无意义\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9\n",
    ")\n",
    "# 将历史记录保存到字典中\n",
    "history = {\"train_loss\": [], \"eval_loss\": [], \"train_acc\": [], \"eval_acc\": []}\n",
    "\n",
    "# 主循环\n",
    "for epoch in range(1, 4):\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc, hist_loss, hist_acc = train(\n",
    "        model, optimizer, dataloader_train, loss_fn, epoch\n",
    "    )\n",
    "    history[\"train_loss\"] += hist_loss\n",
    "    history[\"train_acc\"] += hist_acc\n",
    "    end_time = time.time()\n",
    "    val_loss, val_acc, hist_loss, hist_acc = evaluate(\n",
    "        model, dataloader_eval, loss_fn\n",
    "    )\n",
    "    history[\"eval_loss\"] += hist_loss\n",
    "    history[\"eval_acc\"] += hist_acc\n",
    "    print(\n",
    "        f\"Epoch: {epoch}, Train loss: {train_loss:.3f},\"\n",
    "        f\"Train acc: {train_acc:.3f}, Val loss: {val_loss:.3f}, \"\n",
    "        f\"Val acc: {val_acc:.3f} \"\n",
    "        f\"Epoch time = {(end_time - start_time):.3f}s\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 可视化注意力\n",
    "fig = plt.figure(figsize=(10.0, 10.0))\n",
    "images = (\n",
    "    model.decoder.decoder_blocks[0]\n",
    "    .cross_attention.attention_weights[0, ...]\n",
    "    .detach()\n",
    "    .numpy()\n",
    ")\n",
    "grid = ImageGrid(\n",
    "    fig,\n",
    "    111,  # 类似于 subplot(111)\n",
    "    nrows_ncols=(2, 2),  # 创建2x2的坐标轴网格\n",
    "    axes_pad=0.1,  # 坐标轴之间的间距（以英寸为单位）\n",
    ")\n",
    "for ax, im in zip(grid, images):\n",
    "    # 遍历网格返回Axes。\n",
    "    ax.imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 测试模型\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTranslator\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, transformer):\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "# 测试模型\n",
    "class Translator(nn.Module):\n",
    "    def __init__(self, transformer):\n",
    "        super().__init__()\n",
    "        self.transformer = transformer\n",
    "\n",
    "    @staticmethod\n",
    "    def str_to_tokens(s):\n",
    "        return [ord(z) - 97 + 3 for z in s]\n",
    "\n",
    "    @staticmethod\n",
    "    def tokens_to_str(tokens):\n",
    "        return \"\".join([chr(x + 94) for x in tokens])\n",
    "\n",
    "    def __call__(self, sentence, max_length=None, pad=False):\n",
    "        x = torch.tensor(self.str_to_tokens(sentence))\n",
    "        x = torch.cat(\n",
    "            [torch.tensor([SOS_IDX]), x, torch.tensor([EOS_IDX])]\n",
    "        ).unsqueeze(0)\n",
    "\n",
    "        encoder_output, mask = self.transformer.encode(x)  # (B, S, E)\n",
    "\n",
    "        if not max_length:\n",
    "            max_length = x.size(1)\n",
    "\n",
    "        outputs = (\n",
    "            torch.ones((x.size()[0], max_length)).type_as(x).long() * SOS_IDX\n",
    "        )\n",
    "\n",
    "        for step in range(1, max_length):\n",
    "            y = outputs[:, :step]\n",
    "            probs = self.transformer.decode(y, encoder_output)\n",
    "            output = torch.argmax(probs, dim=-1)\n",
    "            print(f\"Knowing {y} we output {output[:, -1]}\")\n",
    "            if output[:, -1].detach().numpy() in (EOS_IDX, SOS_IDX):\n",
    "                break\n",
    "            outputs[:, step] = output[:, -1]\n",
    "\n",
    "        return self.tokens_to_str(outputs[0])\n",
    "\n",
    "translator = Translator(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'translator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 注意，模型不支持小写字母意外的内容，包括大写字母，空格等\u001b[39;00m\n\u001b[1;32m      2\u001b[0m sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhelloworld\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mtranslator\u001b[49m(sentence)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(out)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'translator' is not defined"
     ]
    }
   ],
   "source": [
    "# 注意，模型不支持小写字母意外的内容，包括大写字母，空格等\n",
    "sentence = \"helloworld\"\n",
    "out = translator(sentence)\n",
    "print(out)\n",
    "\n",
    "# 创建一个子图对象\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10.0, 10.0))\n",
    "# 设置y轴刻度\n",
    "ax.set_yticks(range(len(out)))\n",
    "# 设置x轴刻度\n",
    "ax.set_xticks(range(len(sentence)))\n",
    "\n",
    "# 将x轴标签位置设置在顶部\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "\n",
    "# 设置x轴刻度标签\n",
    "ax.set_xticklabels(iter(sentence))\n",
    "# 设置y轴刻度标签\n",
    "ax.set_yticklabels([f\"step {i}\" for i in range(len(out))])\n",
    "# 显示图像\n",
    "ax.imshow(images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robotic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
